#!/bin/bash

# =============================================================================
# Kind (Kubernetes In Docker) Development Environment Setup
# =============================================================================
# This script creates a complete local development environment using Kind
# with OIDC integration, ArgoCD, Prometheus, and all services
# =============================================================================

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Configuration
CLUSTER_NAME="dev-local"
KUBECONFIG_PATH="$HOME/.kube/config"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"

# Prerequisites check
REQUIRED_TOOLS=(
    "kind"
    "kubectl"
    "helm"
    "docker"
    "git"
    "curl"
)

# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

log() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')] $1${NC}"
}

warn() {
    echo -e "${YELLOW}[$(date +'%Y-%m-%d %H:%M:%S')] WARNING: $1${NC}"
}

error() {
    echo -e "${RED}[$(date +'%Y-%m-%d %H:%M:%S')] ERROR: $1${NC}"
    exit 1
}

info() {
    echo -e "${BLUE}[$(date +'%Y-%m-%d %H:%M:%S')] INFO: $1${NC}"
}

step() {
    echo -e "\n${PURPLE}========================================${NC}"
    echo -e "${PURPLE}STEP: $1${NC}"
    echo -e "${PURPLE}========================================${NC}\n"
}

wait_for_pods() {
    local namespace=$1
    local timeout=${2:-300}
    info "Waiting for pods in namespace '$namespace' to be ready (timeout: ${timeout}s)..."
    kubectl wait --for=condition=ready pod --all -n "$namespace" --timeout="${timeout}s" || {
        warn "Some pods in namespace '$namespace' are not ready yet"
        kubectl get pods -n "$namespace"
    }
}

wait_for_deployment() {
    local deployment=$1
    local namespace=$2
    local timeout=${3:-300}
    info "Waiting for deployment '$deployment' in namespace '$namespace' to be ready..."
    kubectl wait --for=condition=available --timeout="${timeout}s" deployment/"$deployment" -n "$namespace"
}

# Wait for StatefulSet to be ready
wait_for_statefulset() {
    local statefulset=$1
    local namespace=$2
    local timeout=${3:-300}
    info "Waiting for statefulset '$statefulset' in namespace '$namespace' to be ready..."
    kubectl wait --for=condition=ready --timeout="${timeout}s" pod -l app="$statefulset" -n "$namespace"
}

# Copy template file with variable substitution
copy_template() {
    local template_file=$1
    local destination_file=$2
    local substitute_vars=${3:-false}
    
    if [ ! -f "$template_file" ]; then
        error "Template file not found: $template_file"
    fi
    
    # Create destination directory if it doesn't exist
    mkdir -p "$(dirname "$destination_file")"
    
    if [ "$substitute_vars" = "true" ]; then
        # Perform variable substitution
        sed "s|__PROJECT_ROOT__|$PROJECT_ROOT|g" "$template_file" > "$destination_file"
    else
        # Simple copy
        cp "$template_file" "$destination_file"
    fi
    
    log "âœ“ Copied template: $(basename "$template_file") -> $(basename "$destination_file")"
}

# Retry function with exponential backoff
retry_with_backoff() {
    local max_attempts=$1
    shift
    local cmd="$@"
    local attempt=1
    local delay=1
    
    while [ $attempt -le $max_attempts ]; do
        info "Attempt $attempt/$max_attempts: $cmd"
        if eval "$cmd"; then
            log "âœ“ Command succeeded on attempt $attempt"
            return 0
        fi
        
        if [ $attempt -eq $max_attempts ]; then
            error "Command failed after $max_attempts attempts: $cmd"
        fi
        
        warn "Command failed on attempt $attempt. Retrying in ${delay}s..."
        sleep $delay
        attempt=$((attempt + 1))
        delay=$((delay * 2))
    done
}

# Enhanced wait for pods function with better error reporting
wait_for_pods_enhanced() {
    local namespace=$1
    local timeout=${2:-300}
    local selector=${3:-""}
    
    info "Waiting for pods in namespace '$namespace' to be ready (timeout: ${timeout}s)..."
    
    # Build kubectl command
    local kubectl_cmd="kubectl wait --for=condition=ready pod --all -n $namespace --timeout=${timeout}s"
    if [ -n "$selector" ]; then
        kubectl_cmd="kubectl wait --for=condition=ready pod -l $selector -n $namespace --timeout=${timeout}s"
    fi
    
    if ! $kubectl_cmd; then
        warn "Some pods in namespace '$namespace' are not ready yet"
        info "Pod status in namespace '$namespace':"
        kubectl get pods -n "$namespace" -o wide || true
        info "Recent events in namespace '$namespace':"
        kubectl get events -n "$namespace" --sort-by='.lastTimestamp' | tail -10 || true
        return 1
    fi
    
    log "âœ“ All pods in namespace '$namespace' are ready"
}

# Check if namespace exists
namespace_exists() {
    local namespace=$1
    kubectl get namespace "$namespace" >/dev/null 2>&1
}

# Create namespace if it doesn't exist
ensure_namespace() {
    local namespace=$1
    if ! namespace_exists "$namespace"; then
        info "Creating namespace: $namespace"
        kubectl create namespace "$namespace" || error "Failed to create namespace: $namespace"
    else
        info "Namespace '$namespace' already exists"
    fi
}

# Load Docker image into Kind cluster (helper for image pull issues)
load_image_to_kind() {
    local image=$1
    local cluster_name=${2:-$CLUSTER_NAME}
    
    info "Loading image '$image' into Kind cluster '$cluster_name'..."
    if docker image inspect "$image" >/dev/null 2>&1; then
        kind load docker-image "$image" --name "$cluster_name"
        log "âœ“ Image '$image' loaded into Kind cluster"
    else
        warn "Image '$image' not found locally. Pull it first with: docker pull $image"
        return 1
    fi
}

# =============================================================================
# PREREQUISITES CHECK
# =============================================================================

check_prerequisites() {
    step "Checking prerequisites"
    
    local missing_tools=()
    for tool in "${REQUIRED_TOOLS[@]}"; do
        if ! command -v "$tool" &> /dev/null; then
            missing_tools+=("$tool")
        else
            log "âœ“ $tool is installed"
        fi
    done
    
    if [ ${#missing_tools[@]} -ne 0 ]; then
        error "Missing required tools: ${missing_tools[*]}
Please install them first:
- kind: https://kind.sigs.k8s.io/docs/user/quick-start/#installation
- kubectl: https://kubernetes.io/docs/tasks/tools/install-kubectl/
- helm: https://helm.sh/docs/intro/install/
- docker: https://docs.docker.com/get-docker/
- git: https://git-scm.com/downloads"
    fi
    
    # Check Docker is running
    if ! docker info &> /dev/null; then
        error "Docker is not running. Please start Docker Desktop."
    fi
    log "âœ“ Docker is running"
    
    # Check if cluster already exists
    if kind get clusters | grep -q "^$CLUSTER_NAME$"; then
        warn "Kind cluster '$CLUSTER_NAME' already exists"
        read -p "Do you want to delete and recreate it? (y/N): " -r
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            kind delete cluster --name "$CLUSTER_NAME"
            log "âœ“ Deleted existing cluster"
        else
            info "Using existing cluster"
            return
        fi
    fi
}

# =============================================================================
# KIND CLUSTER CONFIGURATION
# =============================================================================

create_kind_config() {
    step "Verifying Kind cluster configuration"
    
    # Substitute variables in the existing config file
    if [ -f "$PROJECT_ROOT/configs/kind-config.yaml" ]; then
        # Perform variable substitution in place
        sed -i.bak "s|__PROJECT_ROOT__|$PROJECT_ROOT|g" "$PROJECT_ROOT/configs/kind-config.yaml"
        rm -f "$PROJECT_ROOT/configs/kind-config.yaml.bak"
        log "âœ“ Kind configuration variables updated"
    else
        error "Kind configuration not found at $PROJECT_ROOT/configs/kind-config.yaml"
    fi
}

# =============================================================================
# KIND CLUSTER CREATION
# =============================================================================

create_kind_cluster() {
    step "Creating Kind cluster"
    
    if ! kind get clusters | grep -q "^$CLUSTER_NAME$"; then
        info "Creating Kind cluster with OIDC configuration..."
        retry_with_backoff 3 "kind create cluster --config '$PROJECT_ROOT/configs/kind-config.yaml'"
        log "âœ“ Kind cluster '$CLUSTER_NAME' created"
    else
        log "âœ“ Kind cluster '$CLUSTER_NAME' already exists"
    fi
    
    # Set kubectl context and verify cluster is accessible
    info "Setting kubectl context and verifying cluster access..."
    retry_with_backoff 3 "kubectl cluster-info --context 'kind-$CLUSTER_NAME'"
    
    log "âœ“ Cluster ready"
}

# =============================================================================
# INGRESS NGINX SETUP
# =============================================================================

install_ingress_nginx() {
    step "Installing Ingress NGINX"
    
    # Install ingress-nginx
    info "Deploying Ingress NGINX controller..."
    retry_with_backoff 3 "kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml"
    
    # Wait for ingress controller deployment to be ready
    info "Waiting for Ingress NGINX controller deployment to be ready..."
    wait_for_deployment "ingress-nginx-controller" "ingress-nginx" 300
    
    # Wait for admission webhook to be ready (critical for ingress creation)
    info "Waiting for Ingress NGINX admission webhook to be ready..."
    local webhook_ready=false
    local max_attempts=60  # Increased from 30 to 60 (5 minutes total)
    local attempt=1
    
    while [ $attempt -le $max_attempts ] && [ "$webhook_ready" = false ]; do
        info "Checking admission webhook readiness (attempt $attempt/$max_attempts)..."
        
        # Check if validating webhook configuration exists
        if ! kubectl get validatingwebhookconfiguration ingress-nginx-admission >/dev/null 2>&1; then
            warn "ValidatingWebhookConfiguration not found yet, waiting 5s..."
            sleep 5
            attempt=$((attempt + 1))
            continue
        fi
        
        # Check if admission webhook service exists and has endpoints
        if ! kubectl get service -n ingress-nginx ingress-nginx-controller-admission >/dev/null 2>&1; then
            warn "Admission webhook service not found yet, waiting 5s..."
            sleep 5
            attempt=$((attempt + 1))
            continue
        fi
        
        # Check if admission webhook pod is ready
        if ! kubectl get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/component=controller -o jsonpath='{.items[0].status.phase}' | grep -q "Running"; then
            warn "Admission webhook pod not running yet, waiting 5s..."
            sleep 5
            attempt=$((attempt + 1))
            continue
        fi
        
        # Check if service has endpoints (pod is actually serving)
        local endpoints
        endpoints=$(kubectl get endpoints -n ingress-nginx ingress-nginx-controller-admission -o jsonpath='{.subsets[*].addresses[*].ip}' 2>/dev/null || echo "")
        if [ -z "$endpoints" ]; then
            warn "Admission webhook service has no endpoints yet, waiting 5s..."
            sleep 5
            attempt=$((attempt + 1))
            continue
        fi
        
        # Final test: try to actually contact the webhook service
        info "Testing webhook connectivity..."
        if kubectl exec -n ingress-nginx deployment/ingress-nginx-controller -- nc -z ingress-nginx-controller-admission.ingress-nginx.svc.cluster.local 443 2>/dev/null; then
            # Give webhook service extra time to be fully ready for TLS
            info "Webhook service is responding"
            sleep 15
            webhook_ready=true
            log "âœ“ Ingress NGINX admission webhook is ready"
        else
            warn "Webhook service not responding yet, waiting 10s..."
            sleep 10
            attempt=$((attempt + 1))
        fi
    done
    
    if [ "$webhook_ready" = false ]; then
        warn "Admission webhook may not be fully ready after $max_attempts attempts"
        info "You may need to retry Dex installation if it fails due to webhook issues"
        info "To manually check webhook status:"
        info "  kubectl get pods -n ingress-nginx"
        info "  kubectl get endpoints -n ingress-nginx ingress-nginx-controller-admission"
    fi
    
    log "âœ“ Ingress NGINX installed and ready"
}

# =============================================================================
# COREDNS LOCALHOST CONFIGURATION
# =============================================================================

configure_localhost_dns() {
    step "Configuring CoreDNS for .localhost domain resolution"
    
    # Wait for ingress controller service to be ready and have a ClusterIP
    info "Waiting for ingress controller service to be ready..."
    local ingress_ip=""
    local max_attempts=30
    local attempt=1
    
    while [ $attempt -le $max_attempts ] && [ -z "$ingress_ip" ]; do
        info "Waiting for ingress controller service IP (attempt $attempt/$max_attempts)..."
        ingress_ip=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.spec.clusterIP}' 2>/dev/null || echo "")
        
        if [ -n "$ingress_ip" ] && [ "$ingress_ip" != "None" ]; then
            break
        fi
        
        sleep 5
        attempt=$((attempt + 1))
    done
    
    if [ -z "$ingress_ip" ] || [ "$ingress_ip" = "None" ]; then
        error "Could not get ingress controller ClusterIP after $max_attempts attempts"
    fi
    
    info "Ingress controller ClusterIP: $ingress_ip"
    
    # Create CoreDNS configuration patch
    info "Patching CoreDNS to resolve .localhost domains via ingress controller..."
    
    # Create temporary patch file
    cat > /tmp/coredns-patch.yaml << EOF
data:
  Corefile: |
    .:53 {
        errors
        health {
            lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure
            fallthrough in-addr.arpa ip6.arpa
            ttl 30
        }
        hosts {
            ${ingress_ip} dex.localhost
            ${ingress_ip} argocd.localhost
            ${ingress_ip} grafana.localhost
            ${ingress_ip} forgejo.localhost
            fallthrough
        }
        prometheus :9153
        forward . /etc/resolv.conf {
            max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
EOF
    
    # Apply CoreDNS configuration with retry
    retry_with_backoff 3 "kubectl patch configmap coredns -n kube-system --patch-file /tmp/coredns-patch.yaml"
    
    # Restart CoreDNS deployment to pick up new configuration
    info "Restarting CoreDNS deployment..."
    kubectl rollout restart deployment coredns -n kube-system
    kubectl rollout status deployment coredns -n kube-system --timeout=300s
    
    # Wait a moment for DNS to propagate
    info "Waiting for DNS configuration to propagate..."
    sleep 10
    
    # Clean up temporary files
    rm -f /tmp/coredns-patch.yaml
    
    log "âœ“ CoreDNS configured to resolve .localhost domains via ingress ($ingress_ip)"
}

# =============================================================================
# DEX OIDC CONFIGURATION
# =============================================================================

setup_dex_oidc() {
    step "Setting up Dex OIDC"
    
    # Verify Dex configuration exists
    if [ ! -f "$PROJECT_ROOT/configs/dex-values.yaml" ]; then
        error "Dex configuration not found at $PROJECT_ROOT/configs/dex-values.yaml"
    fi
    
    # Check if local chart exists, otherwise use remote repo
    local dex_chart=""
    if [ -f "$PROJECT_ROOT/charts/setup/dex-"*.tgz ]; then
        dex_chart=$(ls "$PROJECT_ROOT/charts/setup/dex-"*.tgz | head -1)
        info "Using local Dex chart: $dex_chart"
    else
        # Add Dex repo with retry
        info "Adding Dex Helm repository..."
        retry_with_backoff 3 "helm repo add dex https://charts.dexidp.io"
        retry_with_backoff 3 "helm repo update"
        dex_chart="dex/dex"
        info "Using remote Dex chart: $dex_chart"
    fi
    
    # Create dex namespace
    ensure_namespace "dex"
    
    # Install Dex with retry
    info "Installing Dex OIDC provider..."
    local dex_install_cmd="helm upgrade --install dex '$dex_chart' -n dex --values '$PROJECT_ROOT/configs/dex-values.yaml' --timeout 5m"
    
    # Use retry with longer delay for webhook-related issues
    local attempt=1
    local max_attempts=5
    local delay=10
    
    while [ $attempt -le $max_attempts ]; do
        info "Attempt $attempt/$max_attempts: Installing Dex..."
        
        if eval "$dex_install_cmd"; then
            log "âœ“ Dex installation succeeded on attempt $attempt"
            break
        fi
        
        if [ $attempt -eq $max_attempts ]; then
            error "Dex installation failed after $max_attempts attempts. Check if nginx admission webhook is ready."
        fi
        
        warn "Dex installation failed on attempt $attempt (likely webhook timing). Retrying in ${delay}s..."
        sleep $delay
        attempt=$((attempt + 1))
        
        # Check webhook status for better error messaging
        if ! kubectl get validatingwebhookconfiguration ingress-nginx-admission >/dev/null 2>&1; then
            warn "Nginx admission webhook configuration not found. Webhook may not be ready."
        fi
    done
    
    # Wait for Dex to be ready
    info "Waiting for Dex deployment to be ready..."
    if ! wait_for_deployment "dex" "dex" 300; then
        error "Dex deployment failed to become ready"
    fi
    
    log "âœ“ Dex OIDC installed and configured"
}

# =============================================================================
# OPENLDAP SETUP
# =============================================================================

setup_openldap() {
    step "Setting up OpenLDAP"
    
    # Verify OpenLDAP configuration exists
    if [ ! -f "$PROJECT_ROOT/configs/openldap-values.yaml" ]; then
        error "OpenLDAP configuration not found at $PROJECT_ROOT/configs/openldap-values.yaml"
    fi
    
    # Check if local chart exists, otherwise use remote repo
    local openldap_chart=""
    if [ -f "$PROJECT_ROOT/charts/setup/openldap-"*.tgz ]; then
        openldap_chart=$(ls "$PROJECT_ROOT/charts/setup/openldap-"*.tgz | head -1)
        info "Using local OpenLDAP chart: $openldap_chart"
    else
        # Add helm-openldap repo for OpenLDAP with retry
        info "Adding OpenLDAP Helm repository..."
        retry_with_backoff 3 "helm repo add helm-openldap https://jp-gouin.github.io/helm-openldap"
        retry_with_backoff 3 "helm repo update"
        openldap_chart="helm-openldap/openldap"
        info "Using remote OpenLDAP chart: $openldap_chart"
    fi
    
    # Create ldap namespace
    ensure_namespace "ldap"
    
    # Install OpenLDAP with retry
    info "Installing OpenLDAP server..."
    retry_with_backoff 3 "helm upgrade --install openldap '$openldap_chart' -n ldap --values '$PROJECT_ROOT/configs/openldap-values.yaml'"
    
    # Wait for OpenLDAP to be ready (it's a StatefulSet, not a Deployment)
    info "Waiting for OpenLDAP statefulset to be ready..."
    if ! wait_for_statefulset "openldap" "ldap" 300; then
        warn "OpenLDAP statefulset not ready yet, but continuing..."
        info "Check OpenLDAP pod status: kubectl get pods -n ldap"
        info "Check OpenLDAP logs: kubectl logs -n ldap openldap-0"
    fi
    
    log "âœ“ OpenLDAP installed and configured"
    
    # Display LDAP users information
    info "LDAP Users configured:"
    info "  - admin@local.dev / password (LDAP binding user)"
    info "  - dev1@local.dev / password (super-admins group - cluster-admin)"
    info "  - dev2@local.dev / password (admins group - cluster-admin)"
    info "  - user1@local.dev / password (users group - read-only via authenticated users)"
}

# =============================================================================
# PROMETHEUS MONITORING SETUP
# =============================================================================

setup_prometheus() {
    step "Setting up Prometheus monitoring stack"
    
    # Verify Prometheus configuration exists
    if [ ! -f "$PROJECT_ROOT/configs/prometheus-values.yaml" ]; then
        error "Prometheus configuration not found at $PROJECT_ROOT/configs/prometheus-values.yaml"
    fi
    
    # Check if local chart exists, otherwise use remote repo
    local prometheus_chart=""
    if [ -f "$PROJECT_ROOT/charts/setup/kube-prometheus-stack-"*.tgz ]; then
        prometheus_chart=$(ls "$PROJECT_ROOT/charts/setup/kube-prometheus-stack-"*.tgz | head -1)
        info "Using local Prometheus chart: $prometheus_chart"
    else
        # Add Prometheus community repo
        info "Adding Prometheus community Helm repository..."
        retry_with_backoff 3 "helm repo add prometheus-community https://prometheus-community.github.io/helm-charts"
        retry_with_backoff 3 "helm repo update"
        prometheus_chart="prometheus-community/kube-prometheus-stack"
        info "Using remote Prometheus chart: $prometheus_chart"
    fi
    
    # Create monitoring namespace
    ensure_namespace "monitoring"

    # Install Prometheus stack with retry
    info "Installing Prometheus monitoring stack (this may take a few minutes)..."
    retry_with_backoff 3 "helm upgrade --install prometheus '$prometheus_chart' -n monitoring --values '$PROJECT_ROOT/configs/prometheus-values.yaml' --timeout 15m"
    
    # Wait for Prometheus components to be ready
    info "Waiting for Prometheus operator to be ready..."
    if ! wait_for_deployment "prometheus-kube-prometheus-prometheus-operator" "monitoring" 300; then
        warn "Prometheus operator deployment not ready, but continuing..."
    fi
    
    info "Waiting for Prometheus server to be ready..."
    # Prometheus uses StatefulSet, so we need to wait differently
    kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=prometheus -n monitoring --timeout=300s || warn "Prometheus server not ready yet"
    
    info "Waiting for Grafana to be ready..."
    if ! wait_for_deployment "prometheus-grafana" "monitoring" 300; then
        warn "Grafana deployment not ready, but continuing..."
    fi
    
    # Note: No custom ServiceMonitors needed - kube-prometheus-stack includes built-in monitoring
    info "Using built-in ServiceMonitors for comprehensive Kubernetes monitoring..."
    
    log "âœ“ Prometheus monitoring stack installed and configured"
    
    # Display monitoring access information
    info "Monitoring Access Information:"
    info "  - Grafana Dashboard: https://grafana.localhost (OIDC authentication required)"
    info "  - Prometheus UI: kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090"
    info "  - AlertManager UI: kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-alertmanager 9093:9093"
    info ""
    info "Built-in Kubernetes Monitoring:"
    info "  âœ“ Pod CPU/Memory usage (from kubelet/cAdvisor)"
    info "  âœ“ Node resources (CPU, memory, disk, network)"
    info "  âœ“ Kubernetes objects (deployments, services, ingress)"
    info "  âœ“ Cluster health (API server, etcd, controller manager)"
    info "  âœ“ Container metrics (restarts, status, resource limits)"
    info "  âš  Application-specific metrics require custom /metrics endpoints"
}

# =============================================================================
# ARGOCD SETUP
# =============================================================================

setup_argocd() {
    step "Setting up ArgoCD"
    
    # Verify ArgoCD configuration exists
    if [ ! -f "$PROJECT_ROOT/configs/argocd-values.yaml" ]; then
        error "ArgoCD configuration not found at $PROJECT_ROOT/configs/argocd-values.yaml"
    fi
    
    # Check if local chart exists, otherwise use remote repo
    local argocd_chart=""
    if [ -f "$PROJECT_ROOT/charts/setup/argo-cd-"*.tgz ]; then
        argocd_chart=$(ls "$PROJECT_ROOT/charts/setup/argo-cd-"*.tgz | head -1)
        info "Using local ArgoCD chart: $argocd_chart"
    else
        # Add ArgoCD repo
        info "Adding ArgoCD Helm repository..."
        retry_with_backoff 3 "helm repo add argo https://argoproj.github.io/argo-helm"
        retry_with_backoff 3 "helm repo update"
        argocd_chart="argo/argo-cd"
        info "Using remote ArgoCD chart: $argocd_chart"
    fi
    
    # Create argocd namespace
    ensure_namespace "argocd"
    
    # Note: TLS secrets not needed for HTTP-only setup
    
    # Install ArgoCD with retry
    info "Installing ArgoCD ..."
    retry_with_backoff 3 "helm upgrade --install argocd '$argocd_chart' -n argocd --values '$PROJECT_ROOT/configs/argocd-values.yaml' --timeout 10m"
    
    # Wait for ArgoCD components to be ready
    info "Waiting for ArgoCD server to be ready..."
    if ! wait_for_deployment "argocd-server" "argocd" 300; then
        warn "ArgoCD server deployment not ready, but continuing..."
    fi
    
    info "Waiting for ArgoCD application controller to be ready..."
    if ! wait_for_deployment "argocd-application-controller" "argocd" 300; then
        warn "ArgoCD application controller not ready, but continuing..."
    fi
    
    info "Waiting for ArgoCD repo server to be ready..."
    if ! wait_for_deployment "argocd-repo-server" "argocd" 300; then
        warn "ArgoCD repo server not ready, but continuing..."
    fi
    
    log "âœ“ ArgoCD installed and configured"
    
    # Display ArgoCD access information
    info "ArgoCD Access Information:"
    info "  - ArgoCD UI: https://argocd.localhost (OIDC authentication required)"
    info "  - CLI Login: argocd login argocd.localhost --sso"
    info "  - Port-forward: kubectl port-forward -n argocd svc/argocd-server 8080:80"
}

# =============================================================================
# FORGEJO SETUP
# =============================================================================

setup_forgejo() {
    step "Setting up Forgejo for Git platform"
    
    # Verify Forgejo configuration exists
    if [ ! -f "$PROJECT_ROOT/configs/forgejo-values.yaml" ]; then
        error "Forgejo configuration not found at $PROJECT_ROOT/configs/forgejo-values.yaml"
    fi
    
    # Check if local chart exists, otherwise use remote repo
    local forgejo_chart=""
    if [ -f "$PROJECT_ROOT/charts/setup/forgejo-"*.tgz ]; then
        forgejo_chart=$(ls "$PROJECT_ROOT/charts/setup/forgejo-"*.tgz | head -1)
        info "Using local Forgejo chart: $forgejo_chart"
    else
        forgejo_chart="oci://code.forgejo.org/forgejo-helm/forgejo"
        info "Using remote Forgejo chart: $forgejo_chart"
    fi
    
    # Create forgejo namespace
    ensure_namespace "forgejo"
    
    # Install Forgejo with retry
    info "Installing Forgejo (this may take a few minutes)..."
    retry_with_backoff 3 "helm upgrade --install forgejo '$forgejo_chart' -n forgejo --values '$PROJECT_ROOT/configs/forgejo-values.yaml' --timeout 10m"
    
    # Wait for Forgejo components to be ready
    info "Waiting for Forgejo deployment to be ready..."
    if ! wait_for_deployment "forgejo" "forgejo" 300; then
        warn "Forgejo deployment not ready, but continuing..."
    fi
    
    info "Waiting for PostgreSQL to be ready..."
    if ! wait_for_deployment "forgejo-postgresql" "forgejo" 300; then
        warn "Forgejo PostgreSQL not ready, but continuing..."
    fi
    
    # Wait a bit for services to be fully ready
    info "Waiting for services to be fully ready..."
    sleep 30
    
    # Login to Helm registry
    info "Logging in to Forgejo Helm registry..."
    if command -v helm >/dev/null 2>&1; then
        if helm registry login forgejo.localhost --username platform-admin --password password --insecure >/dev/null 2>&1; then
            log "âœ“ Helm registry login successful"
        else
            warn "Helm registry login failed, you may need to login manually"
        fi
    else
        warn "Helm not found, skipping registry login"
    fi
    
    log "âœ“ Forgejo installed and configured"
    
    # Display Forgejo access information
    info "Forgejo Access Information:"
    info "  - Forgejo UI: http://forgejo.localhost (OIDC authentication available)"
    info "  - Direct access: kubectl port-forward -n forgejo svc/forgejo 3000:3000"
    info "  - Default admin user: platform-admin / password (if needed for initial setup)"
    info ""
    info "OIDC Authentication:"
    info "  âœ“ Users can sign in with Dex OIDC"
    info "  âœ“ Auto-registration enabled for OIDC users"
    info "  âœ“ super-admins group gets admin privileges"
    info "  âœ“ users group gets restricted access"
}

# =============================================================================
# MAIN EXECUTION
# =============================================================================

main() {
    # Execute all setup steps
    check_prerequisites
    create_kind_config
    create_kind_cluster
    install_ingress_nginx
    configure_localhost_dns
    setup_openldap
    setup_dex_oidc
    setup_prometheus
    setup_argocd
    setup_forgejo
    
    # Display final summary
    echo -e "\n${GREEN}========================================${NC}"
    echo -e "${GREEN}ğŸ‰ Local Development Environment Ready!${NC}"
    echo -e "${GREEN}========================================${NC}"
    echo -e ""
    echo -e "${CYAN}Access Information:${NC}"
    echo -e "  ğŸ”— OIDC Discovery: ${BLUE}http://dex.localhost/.well-known/openid-configuration${NC}"
    echo -e "  ğŸ“Š Grafana Dashboard: ${BLUE}http://grafana.localhost${NC} (OIDC required)"
    echo -e "  ğŸš€ ArgoCD: ${BLUE}http://argocd.localhost${NC} (OIDC required)"
    echo -e "  ğŸ™ Git Platform: ${BLUE}http://forgejo.localhost${NC} (OIDC available)"
    echo -e ""
    echo -e "${CYAN}Test Users (password: 'password' for all):${NC}"
    echo -e "  ğŸ‘‘ Dev Admin 1: ${YELLOW}dev1@local.dev${NC} (super-admin)"
    echo -e "  ğŸ‘‘ Dev Admin 2: ${YELLOW}dev2@local.dev${NC} (admin)"
    echo -e "  ğŸ‘¤ Regular User: ${YELLOW}user1@local.dev${NC}"
    echo -e ""
    echo -e "${CYAN}Helm Package Registry:${NC}"
    echo -e "  ğŸ“¦ Push Helm Chart: ${BLUE}helm push mychart-1.0.0.tgz oci://forgejo.localhost/forge --plain-http${NC}"
    echo -e "  ğŸ“¥ Pull Helm Chart: ${BLUE}helm pull oci://forgejo.localhost/forge/mychart --version 1.0.0 --plain-http${NC}"
    echo -e "  ğŸ” Registry Login: ${BLUE}helm registry login forgejo.localhost --username platform-admin --password password --insecure${NC}"
    echo -e "  ğŸŒ Package Registry UI: ${BLUE}http://forgejo.localhost/forge/-/packages${NC}"
    echo -e ""
    echo -e "${CYAN}Git Repository:${NC}"
    echo -e "  ğŸ“¤ Git Push: ${BLUE}git push http://platform-admin:password@forgejo.localhost/forge/repo.git${NC}"
    echo -e "  ğŸ“¥ Git Clone: ${BLUE}git clone http://@forgejo.localhost/forge/repo.git${NC}"
    echo -e "  ğŸ” Git Remote: ${BLUE}git remote set-url origin http://forgejo.localhost/forge/repo.git${NC}"
    echo -e ""
    echo -e "${CYAN}Useful Commands:${NC}"
    echo -e "  Port-forward Prometheus: ${BLUE}kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090${NC}"
    echo -e "  Port-forward AlertManager: ${BLUE}kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-alertmanager 9093:9093${NC}"
}
# Run main function
main "$@"